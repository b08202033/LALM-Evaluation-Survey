# LALM_Evaluation_Survey

## üîä General Auditory Awareness and Processing 

| Year        | Authors                         | Venue     | Paper |
|--------------|------------------------------|----------|----------|
| 2025  |  Maimon et al.    | ICASSP 2025| [Salmon: A Suite for Acoustic Language Model Evaluation](https://arxiv.org/abs/2409.07437)|
|   2024       |    Gao et al.                      | arXiv    |  [Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models](https://arxiv.org/abs/2412.05167) |
|   2023       |     Seyssel et al.                     |   EMNLP 2024 (Main)  |  [EmphAssess: a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models](https://arxiv.org/abs/2312.14069) |
|   2025       |       Deshmukh et al.                   |  ICLR 2025   |  [ADIFF: Explaining audio difference using natural language](https://arxiv.org/abs/2502.04476) |
|   2023       |       Huang et al.                   |  ICASSP 2024      | [Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech](https://arxiv.org/abs/2309.09510) |
|    2024      |      Huang et al.                     |      arXiv  | [Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks](https://arxiv.org/abs/2411.05361) |
|   2024       |      Yang et al.                    |  ACL 2024   |  [AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension](https://arxiv.org/abs/2402.07729) |
|   2024       |      Wang et al.                    |  arXiv   | [AudioBench: A Universal Benchmark for Audio Large Language Models](https://arxiv.org/abs/2406.16020)  |
|    2024      |       Week et al.                   |   ISMIR 2024     | [MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models](https://arxiv.org/abs/2408.01337) |
|   2025       |       Cao et al.                   |  arXiv   |  [FinAudio: A Benchmark for Audio Large Language Models in Financial Applications](https://arxiv.org/abs/2503.20990) |
|   2024       |      Bu et al.                    |  arXiv   | [Roadmap towards Superhuman Speech Understanding using Large Language Models](https://arxiv.org/abs/2410.13268)  |
|   2024 |       Chen et al.                    |  	EMNLP 2024   | [Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models](https://arxiv.org/abs/2409.18680)  |
|    2025      |  Zang et al.                        |   arXiv  |  [Are you really listening? Boosting Perceptual Awareness in Music-QA Benchmarks](https://arxiv.org/abs/2504.00369)|
|   2024       |    Zhao et al.                      |   arXiv  | [OpenMU: Your Swiss Army Knife for Music Understanding](https://arxiv.org/abs/2410.15573)  |
|   2025       |    Wang et al.                      |     | arXiv  | [Advancing Singlish Understanding: Bridging the Gap with Datasets and Multimodal Models](https://arxiv.org/abs/2501.01034)
|   2025       |        Xue et al.                  | arXiv    | [Audio-FLAN: A Preliminary Release](https://arxiv.org/abs/2502.16584)  | 
|   2025       |              Wang et al.            |    arXiv  | [QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions](https://arxiv.org/abs/2503.20290)|
|  2025        |     Pandy et al.                     |  arXiv    | [SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning](https://arxiv.org/abs/2504.09081)  |
|  2023        |     Gong et al.                     |   ICLR 2024   | [Listen, Think, and Understand](https://arxiv.org/abs/2305.10790)  |
|  2022        |     Lipping et al.                     |  EUSIPCO 2022   | [Clotho-AQA: A Crowdsourced Dataset for Audio Question Answering](https://arxiv.org/abs/2204.09634)  |
|   2024       |       Huang et al.                   |    SynData4GenAI 2024    | [SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning](https://arxiv.org/abs/2408.13891) |
|   2024  |      Wei et al.                    |  arXiv   |  [ASR-EC Benchmark: Evaluating Large Language Models on Chinese ASR Error Correction](https://arxiv.org/abs/2412.03075) |
|   2024 |      Li et al.                    |  SLT 2024    | [WHISMA: A Speech-LLM to Perform Zero-shot Spoken Language Understanding](https://arxiv.org/abs/2408.16423)  | 
|    2024      |      Robinson et al.                    |  arXiv   | [NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics](https://arxiv.org/abs/2411.07186)   |

## üß† Knowledge and Reasoning

| Year        | Authors                         | Venue     | Paper |
|--------------|------------------------------|----------|----------|
|  2020    |   Nguyen et al.     |  Workshop@NeuRIPS 2020  |[The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling](https://arxiv.org/abs/2011.11588)   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |

## üó£Ô∏è Dialogue-oriented Evaluation 

| Year        | Authors                         | Venue     | Paper |
|--------------|------------------------------|----------|----------|
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |


## üõ°Ô∏è Fairness, Safety, and Trustworthiness 

| Year        | Authors                         | Venue     | Paper |
|--------------|------------------------------|----------|----------|
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
|          |                          |     |   |
